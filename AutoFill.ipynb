{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8550189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d47219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Refat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f5387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Refat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e7cf400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "import re\n",
    "import tkinter as tk\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('./corpus.txt', 'r') as file:\n",
    "    corpus = file.read()\n",
    "       \n",
    "corpus = re.sub(r'[.,\\'\"():;â€™-]', '', corpus) #removing special characters\n",
    "corpus = corpus.lower()   \n",
    "tokens = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16c4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the tokens into a bunch of n-grams\n",
    "def ngram(tokens, n):\n",
    "    finalResult = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        myList = tokens[i:i+n]\n",
    "        myTuple = tuple(myList)\n",
    "        finalResult.append(myTuple)\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "# Count the frequency of each n-gram\n",
    "def countFreq(nGrams, n):\n",
    "    nGramFreq = defaultdict(lambda: defaultdict(lambda: 0)) #initialization for a 2d dictionary\n",
    "    for words in nGrams:\n",
    "        start = words[0:n-1]\n",
    "        end = words[-1]\n",
    "        nGramFreq[start][end] += 1\n",
    "    return nGramFreq\n",
    "\n",
    "\n",
    "# Calculate the probability of each n-gram\n",
    "def calcProbability(nGramFreq, n):\n",
    "    for start in nGramFreq:\n",
    "        totalNGrams = float(sum(nGramFreq[start].values()))\n",
    "        for end in nGramFreq[start]:\n",
    "            nGramFreq[start][end] /= totalNGrams\n",
    "    return nGramFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02cb822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTheModel(token, n):\n",
    "    nGrams = ngram(tokens, n)\n",
    "    nGramFreq = countFreq(nGrams, n)\n",
    "    model = calcProbability(nGramFreq, n)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49132acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter n: 5\n",
      "write your words: you\n",
      "1 - can\n",
      "2 - have\n",
      "3 - are\n",
      "4 - know\n",
      "5 - think\n",
      "6 - dont\n",
      "7 - need\n",
      "8 - that\n",
      "9 - and\n",
      "10 - will\n",
      "Want to terminate? y or n : y\n"
     ]
    }
   ],
   "source": [
    "#running n-gram\n",
    "n = input(\"Enter n: \")\n",
    "sentence = input(\"write your words: \") \n",
    "\n",
    "while True:\n",
    "    sentenceTokens = nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    if(len(sentenceTokens) == 1):\n",
    "        n = 2\n",
    "    else:\n",
    "        n = 3\n",
    "\n",
    "    model = createTheModel(tokens, n)\n",
    "    sentenceTokens = tuple(sentenceTokens[-n+1:])\n",
    "    for start in model:\n",
    "        if sentenceTokens == start:\n",
    "            i=1\n",
    "            predictions = sorted(model[start].items(), key = lambda t:(-t[1])) #-t[1] is to sort in a descending order\n",
    "            for predictedWord in predictions:\n",
    "                if i == 11:\n",
    "                    break\n",
    "                print(i,\"-\",predictedWord[0]) \n",
    "                i += 1\n",
    "    answer = input(\"Want to terminate? y or n : \")\n",
    "    if answer == 'y' or answer == 'Y':\n",
    "        break;\n",
    "    sentence += \" \" + input(\">>> \" + sentence + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24eb3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI\n",
    "def suggestNextWord(userInput):\n",
    "    global n\n",
    "    userInput = userInput.lower()\n",
    "    userInputTokens = nltk.word_tokenize(userInput)\n",
    "    if len(userInputTokens) == 1:\n",
    "        n = 2\n",
    "    else:\n",
    "        n = 3\n",
    "    model = createTheModel(tokens, n)\n",
    "    userInputTokens = tuple(userInputTokens[-n+1:]) #take the last n-1 words\n",
    "    for start in model:\n",
    "        if userInputTokens == start:\n",
    "            i = 1\n",
    "            predictions = sorted(model[start].items(), key=lambda t:(-t[1])) #-t[1] is to sort in a descending order\n",
    "            for predictedWord in predictions:\n",
    "                if i == 10:\n",
    "                    break\n",
    "                yield predictedWord[0] #[0] to get the key only not the probability, return it as an object\n",
    "                i += 1\n",
    "\n",
    "\n",
    "def onkeyRelease(event):\n",
    "    global searchBar, output\n",
    "    inputText = searchBar.get()\n",
    "    output.config(state = tk.NORMAL) \n",
    "    output.delete('1.0', \"end\")\n",
    "    if inputText:\n",
    "        if(inputText[-1] == ' '):\n",
    "            inputText = inputText[:-1]\n",
    "        for suggestion in suggestNextWord(inputText):\n",
    "            output.insert(\"end\", inputText + \" \" + suggestion + \"\\n\")\n",
    "    output.config(state = tk.DISABLED) \n",
    "\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"You type, We Complete!\")\n",
    "\n",
    "# Create the search bar\n",
    "searchBar = tk.Entry(window, width = 54)\n",
    "searchBar.pack(pady = 10)    \n",
    "searchBar.bind('<KeyRelease>', onkeyRelease)\n",
    "\n",
    "# Create the output widget\n",
    "output = tk.Text(window, height = 10, width = 70)\n",
    "output.pack()\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf46b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
